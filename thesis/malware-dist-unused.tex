\begin{enumerate}
  \begin{comment}
\item \textbf{Differences between static paths.} For a given pair of markers, the training set must contain examples of all the possible static paths between these two markers.
  \end{comment}
  
\item \textbf{Architectural event differences among dynamic path instances of a given static path.} For a given marker-to-marker static path, the training set must contain a sufficient number of dynamic instances to capture the variation due to architectural events (cache, branch prediction) that can occur on this path.

\item For a given static marker-to-marker path, with the exact same sequence of micro-architectural events/activities, any remaining variation in the signal must be due to:
  \begin{enumerate}
    \item \textbf{Signal Propagation.} The monitored device and the monitor can be modeled as a transmitter and receiver respectively. The measured signal must propagate between this transmitter and receiver. Therefore there is some loss due to the distance between the transmitter and receiver. Other propagation effects such fading, scattering, and diffraction could potentially effect the received signal. 
    \item \textbf{Noise sources.} Noise sources which are asynchronous to the demodulated signal such as narrowband CW sources or broadband noise can be present in the external environment and can be created by the monitored device. These noise sources degrade the received signal. 
    \item \textbf{Non-linear distortions.} Distortions caused by the measurement equipment, amplifiers, etc. can degrade the received signal.
  \end{enumerate}
\end{enumerate}


To optimize the accuracy of ZOP's predictions, we must first determine which effects degrade ZOP's accuracy. We can classify these effects into the following categories:


\begin{comment}
The effect of differences between static paths is likely the largest contributor to ZOPs performance. For example, if a particular transition in the marker graph is possible, but we have no training example for this transition, ZOP is unaware of this possible transition. ZOP is therefore guaranteed to mispredict this section of code. In some cases this may result in complete prediction failure for the remainder of the execution if the predicted and actual execution paths never reconverge. Furthermore, in some cases we may have a training example which transitions between markers A and B, but the training example may consist of a different sequence of basic blocks than the sequence of basic blocks in the to-be-predicted execution. We can enumerate all the static paths (basic block sequences) between any two markers for all pairs of markers A and B in the training input set, and separately for the evaluation input set.  This analysis is out of the scope of this work because it requires the ability to calculate the sequence of basic blocks between all the markers encountered for a given execution.
\end{comment}


\begin{comment}
For this measurement and all the measurements in this chapter, the Training 2 phase of ZOP will be skipped. The Training 2 phase records uninstrumented waveforms and uses time warping to compare the instrumented and uninstrumented training waveforms to infer the timing of markers within the uninstrumented waveforms. Instead, both the training and evaluation measurements will be done with instrumentation active. In realistic usage, time warping would be used because this allows the original program to be run during evaluation with zero instrumentation. However, for the purposes of malware detection, it is plausible that the monitored programs could be required to have some small instrumentation overhead. In such a scenario, the instrumentation used in the monitored programs would \textit{not record marker timing} or any other information, but would perform an activity that mimics the recording of marker timing. This would result in waveforms that will match the instrumented training waveforms, while not using any memory nor requiring any other checks or verification on the processor, therefore still meeting all the requirements for malware detection at a distance. Training 2 was skipped in the interest of saving measurement time, and because the small degradation in performance caused by time warping will be evaluated separately and independently in future work after the effects analyzed in this chapter are understood. 
\end{comment}
